#!/usr/bin/env python3
"""
æ–°é—»å¤„ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ–°é—»çš„AIè¯„ä¼°ã€å†…å®¹è·å–å’Œä¼˜åŒ–å¤„ç†
"""

from typing import Dict, Any, List
from .ai_analyzer import AIContentAnalyzer
from .news_deduplicator import NewsDeduplicator
from .zyte_client import create_zyte_client

def fetch_full_content_with_fallback(url: str, api_config: Dict[str, Any]) -> str:
    """
    ä½¿ç”¨FireCrawl + Zyteå¤‡ç”¨æœºåˆ¶è·å–å®Œæ•´å†…å®¹

    æµç¨‹ï¼š
    1. é¦–å…ˆå°è¯•FireCrawl
    2. FireCrawlå¤±è´¥æ—¶ä½¿ç”¨Zyte (æœ€å¤šé‡è¯•3æ¬¡)
    3. éƒ½å¤±è´¥æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
    """

    # ç¬¬ä¸€æ­¥ï¼šå°è¯•FireCrawlï¼ˆé‡è¯•3æ¬¡ï¼‰
    print(f"[INFO] å°è¯•ä½¿ç”¨FireCrawlæŠ“å–: {url}")
    firecrawl_content = _try_firecrawl(url, api_config, max_retries=3)

    if firecrawl_content:
        print(f"[INFO] FireCrawlæŠ“å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(firecrawl_content)} å­—ç¬¦")
        return firecrawl_content

    # ç¬¬äºŒæ­¥ï¼šFireCrawlå¤±è´¥ï¼Œå°è¯•Zyte
    print(f"[WARN] FireCrawlæŠ“å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Zyteå¤‡ç”¨æŠ“å–...")
    zyte_content = _try_zyte(url, api_config, max_retries=3)

    if zyte_content:
        print(f"[INFO] Zyteå¤‡ç”¨æŠ“å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(zyte_content)} å­—ç¬¦")
        return zyte_content

    # ç¬¬ä¸‰æ­¥ï¼šéƒ½å¤±è´¥äº†
    print(f"[ERROR] FireCrawlå’ŒZyteéƒ½æŠ“å–å¤±è´¥: {url}")
    return ""


def _try_firecrawl(url: str, api_config: Dict[str, Any], max_retries: int = 3) -> str:
    """å°è¯•ä½¿ç”¨FireCrawlæŠ“å–å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
    try:
        from firecrawl import FirecrawlApp

        api_key = api_config.get('firecrawl_api_key', '')
        if not api_key or api_key.startswith('YOUR_'):
            print("[WARN] FireCrawl APIå¯†é’¥æœªé…ç½®")
            return ""

        app = FirecrawlApp(api_key=api_key)

        for attempt in range(max_retries):
            try:
                print(f"[INFO] FireCrawlå°è¯•ç¬¬ {attempt + 1}/{max_retries} æ¬¡...")

                # å°è¯•ä½¿ç”¨scrapeæ¨¡å¼ï¼ˆæ ¹æ®æ–°çš„APIæ ¼å¼ï¼‰
                try:
                    # ä½¿ç”¨æ–°çš„APIæ ¼å¼ï¼šapp.scrape_url(url, formats=['markdown', 'html'])
                    result = app.scrape_url(url, formats=['markdown', 'html'])

                    if result:
                        # ä¼˜å…ˆä½¿ç”¨markdownæ ¼å¼
                        if hasattr(result, 'markdown') and result.markdown:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆmarkdownï¼‰æˆåŠŸ")
                            return result.markdown
                        elif hasattr(result, 'html') and result.html:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆhtmlï¼‰æˆåŠŸ")
                            return result.html
                        elif hasattr(result, 'content') and result.content:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆcontentï¼‰æˆåŠŸ")
                            return result.content
                        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                        elif isinstance(result, dict):
                            if 'markdown' in result and result['markdown']:
                                print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆdict markdownï¼‰æˆåŠŸ")
                                return result['markdown']
                            elif 'html' in result and result['html']:
                                print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆdict htmlï¼‰æˆåŠŸ")
                                return result['html']
                            elif 'content' in result and result['content']:
                                print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆdict contentï¼‰æˆåŠŸ")
                                return result['content']
                except Exception as e:
                    print(f"[WARN] FireCrawl scrapeæ¨¡å¼ï¼ˆæ–°æ ¼å¼ï¼‰å¤±è´¥: {e}")

                # å°è¯•åªä½¿ç”¨markdownæ ¼å¼
                try:
                    result = app.scrape_url(url, formats=['markdown'])

                    if result:
                        if hasattr(result, 'markdown') and result.markdown:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆä»…markdownï¼‰æˆåŠŸ")
                            return result.markdown
                        elif isinstance(result, dict) and 'markdown' in result:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆä»…markdown dictï¼‰æˆåŠŸ")
                            return result['markdown']
                except Exception as e:
                    print(f"[WARN] FireCrawl scrapeæ¨¡å¼ï¼ˆä»…markdownï¼‰å¤±è´¥: {e}")

                # æœ€åå°è¯•ä¸å¸¦æ ¼å¼å‚æ•°çš„è°ƒç”¨
                try:
                    result = app.scrape_url(url)

                    if result:
                        if hasattr(result, 'markdown') and result.markdown:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰æˆåŠŸ")
                            return result.markdown
                        elif hasattr(result, 'content') and result.content:
                            print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆé»˜è®¤contentï¼‰æˆåŠŸ")
                            return result.content
                        elif isinstance(result, dict):
                            if 'markdown' in result and result['markdown']:
                                print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆé»˜è®¤dictï¼‰æˆåŠŸ")
                                return result['markdown']
                            elif 'content' in result and result['content']:
                                print(f"[INFO] FireCrawl scrapeæ¨¡å¼ï¼ˆé»˜è®¤dict contentï¼‰æˆåŠŸ")
                                return result['content']
                except Exception as e:
                    print(f"[WARN] FireCrawl scrapeæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰å¤±è´¥: {e}")

                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿: 1s, 2s, 4s
                    print(f"[INFO] FireCrawlç¬¬ {attempt + 1} æ¬¡å¤±è´¥ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)

            except Exception as e:
                print(f"[ERROR] FireCrawlç¬¬ {attempt + 1} æ¬¡å°è¯•å¼‚å¸¸: {e}")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"[INFO] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)

        print(f"[ERROR] FireCrawlæ‰€æœ‰ {max_retries} æ¬¡å°è¯•éƒ½å¤±è´¥")
        return ""

    except ImportError:
        print("[ERROR] FireCrawlåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install firecrawl-py")
        return ""
    except Exception as e:
        print(f"[ERROR] FireCrawlåˆå§‹åŒ–å¤±è´¥: {e}")
        return ""


def _try_zyte(url: str, api_config: Dict[str, Any], max_retries: int = 3) -> str:
    """å°è¯•ä½¿ç”¨ZyteæŠ“å–å†…å®¹"""
    try:
        zyte_api_key = api_config.get('zyte_api_key', '')
        if not zyte_api_key or zyte_api_key.startswith('YOUR_'):
            print("[WARN] Zyte APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡ZyteæŠ“å–")
            return ""

        # åˆ›å»ºZyteå®¢æˆ·ç«¯
        zyte_client = create_zyte_client(zyte_api_key)
        if not zyte_client:
            print("[ERROR] Zyteå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥")
            return ""

        # ä½¿ç”¨ZyteæŠ“å–å†…å®¹
        content = zyte_client.extract_content(url, max_retries=max_retries)
        return content or ""

    except Exception as e:
        print(f"[ERROR] ZyteæŠ“å–å¼‚å¸¸: {e}")
        return ""


class NewsProcessor:
    """æ–°é—»å¤„ç†å™¨ - è´Ÿè´£æ–°é—»çš„è¯„ä¼°ã€è·å–å’Œä¼˜åŒ–å¤„ç†"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–æ–°é—»å¤„ç†å™¨"""
        self.config = config
        self.ai_analyzer = AIContentAnalyzer(config)
        self.deduplicator = NewsDeduplicator(similarity_threshold=0.6)
        
    def process_news_batch(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æ–°é—»"""
        if not news_list:
            print("[WARN] æ²¡æœ‰æ–°é—»éœ€è¦å¤„ç†")
            return []

        print(f"[INFO] å¼€å§‹æ‰¹é‡å¤„ç† {len(news_list)} æ¡æ–°é—»")

        # æ­¥éª¤0: æ–°é—»å»é‡åˆå¹¶
        deduplicated_news = self.deduplicator.deduplicate_and_merge(news_list)

        # æ­¥éª¤1: AIè¯„ä¼°ç­›é€‰
        relevant_news = self.evaluate_news_relevance(deduplicated_news)

        if not relevant_news:
            print("[WARN] æ²¡æœ‰æ–°é—»é€šè¿‡AIè¯„ä¼°")
            return []

        # æ­¥éª¤2: æŒ‰AIè¯„åˆ†æ’åºï¼Œåªä¿ç•™å‰Næ¡æ–°é—»
        top_news = self.select_top_news_by_score(relevant_news)

        if not top_news:
            print("[WARN] æ²¡æœ‰æ–°é—»è¿›å…¥æœ€ç»ˆæ¨é€åˆ—è¡¨")
            return []

        # æ­¥éª¤3: åªå¯¹æœ€ç»ˆä¿ç•™çš„æ–°é—»è·å–å®Œæ•´å†…å®¹
        content_news = self.fetch_full_content_batch(top_news)

        # æ­¥éª¤4: AIä¼˜åŒ–å’Œç¿»è¯‘
        processed_news = self.optimize_and_translate_batch(content_news)

        print(f"[INFO] æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…± {len(processed_news)} æ¡æ–°é—»å‡†å¤‡å‘å¸ƒ")
        return processed_news
    
    def evaluate_news_relevance(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯„ä¼°æ–°é—»ç›¸å…³æ€§"""
        print("[INFO] æ­¥éª¤1ï¼šç»Ÿä¸€AIè¯„ä¼°æ‰€æœ‰æ–°é—»...")
        relevant_news = []
        min_score = self.config.get('ai_settings', {}).get('min_relevance_score', 6)
        
        for i, news_item in enumerate(news_list, 1):
            try:
                source_type = news_item.get('source_type', 'Unknown')
                title = news_item.get('title', '')[:50]
                print(f"[INFO] AIè¯„ä¼°ç¬¬ {i}/{len(news_list)} æ¡æ–°é—» ({source_type}): {title}...")
                
                # ä½¿ç”¨æ–°çš„AIè¯„ä¼°æ–¹æ³•
                evaluation_result = self.ai_analyzer.evaluate_news_relevance([news_item])

                if evaluation_result and len(evaluation_result) > 0:
                    evaluated_news = evaluation_result[0]
                    is_relevant = evaluated_news.get('is_relevant', False)
                    score = evaluated_news.get('relevance_score', 0)
                else:
                    is_relevant = False
                    score = 0
                
                if is_relevant and score >= min_score:
                    # ä¿å­˜AIè¯„åˆ†åˆ°æ–°é—»é¡¹ä¸­
                    news_item['ai_score'] = score
                    relevant_news.append(news_item)
                    print(f"[INFO] æ–°é—» {i} AIè¯„ä¼°é€šè¿‡ (è¯„åˆ†: {score:.2f})")
                else:
                    print(f"[INFO] æ–°é—» {i} AIè¯„ä¼°ä¸é€šè¿‡ (è¯„åˆ†: {score:.2f})ï¼Œè·³è¿‡")
                    
            except Exception as e:
                print(f"[ERROR] æ–°é—» {i} AIè¯„ä¼°å¤±è´¥: {str(e)}")
                continue
        
        print(f"[INFO] AIè¯„ä¼°å®Œæˆï¼Œ{len(relevant_news)}/{len(news_list)} æ¡æ–°é—»é€šè¿‡è¯„ä¼°")
        return relevant_news

    def select_top_news_by_score(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰AIè¯„åˆ†æ’åºï¼Œé€‰æ‹©æ’åå‰Nçš„æ–°é—»"""
        if not news_list:
            return []

        max_send_limit = self.config.get('max_send_limit', 10)
        print(f"[INFO] æ­¥éª¤2ï¼šæŒ‰AIè¯„åˆ†æ’åºï¼Œé€‰æ‹©å‰ {max_send_limit} æ¡æ–°é—»...")

        # æŒ‰AIè¯„åˆ†é™åºæ’åº
        sorted_news = sorted(news_list, key=lambda x: x.get('ai_score', 0), reverse=True)

        # åªä¿ç•™å‰Næ¡
        top_news = sorted_news[:max_send_limit]

        print(f"[INFO] è¯„åˆ†æ’åºç»“æœ:")
        for i, news in enumerate(top_news, 1):
            title = news.get('title', '')[:40]
            score = news.get('ai_score', 0)
            source = news.get('source_type', 'Unknown')
            print(f"  {i:2d}. [{source}] {title}... (è¯„åˆ†: {score:.2f})")

        if len(sorted_news) > max_send_limit:
            print(f"[INFO] å…±æœ‰ {len(sorted_news)} æ¡æ–°é—»é€šè¿‡è¯„ä¼°ï¼Œä¿ç•™è¯„åˆ†æœ€é«˜çš„ {len(top_news)} æ¡")
        else:
            print(f"[INFO] å…±æœ‰ {len(top_news)} æ¡æ–°é—»è¿›å…¥æœ€ç»ˆå¤„ç†")

        return top_news
    
    def fetch_full_content_batch(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è·å–å®Œæ•´å†…å®¹"""
        # æ£€æŸ¥çˆ¬è™«å¼€å…³
        content_extraction_enabled = self.config.get('content_extraction', {}).get('enabled', True)

        if not content_extraction_enabled:
            print(f"[INFO] æ­¥éª¤3ï¼šå†…å®¹æŠ“å–å·²ç¦ç”¨ï¼Œè·³è¿‡FireCrawl+ZyteæŠ“å–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹...")
            processed_news = []
            for i, news_item in enumerate(news_list, 1):
                # ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºraw_content
                fallback_content = (news_item.get('content', '') or
                                  news_item.get('description', '') or
                                  news_item.get('summary', ''))
                news_item['raw_content'] = fallback_content
                processed_news.append(news_item)
                title = news_item.get('title', '')[:50]
                print(f"[INFO] æ–°é—» {i}/{len(news_list)} ä½¿ç”¨åŸå§‹å†…å®¹: {title}... (é•¿åº¦: {len(fallback_content)})")

            print(f"[INFO] åŸå§‹å†…å®¹å¤„ç†å®Œæˆï¼Œ{len(processed_news)}/{len(news_list)} æ¡æ–°é—»å¤„ç†å®Œæˆ")
            return processed_news

        print(f"[INFO] æ­¥éª¤3ï¼šä½¿ç”¨FireCrawl+Zyteå¤‡ç”¨æœºåˆ¶è·å– {len(news_list)} æ¡æœ€ç»ˆæ–°é—»çš„å®Œæ•´å†…å®¹...")
        processed_news = []

        for i, news_item in enumerate(news_list, 1):
            try:
                title = news_item.get('title', '')[:50]
                print(f"[INFO] å†…å®¹æŠ“å–ç¬¬ {i}/{len(news_list)} æ¡æ–°é—»: {title}...")

                api_config = {
                    'firecrawl_api_key': self.config.get('firecrawl_api_key', ''),
                    'zyte_api_key': self.config.get('zyte_api_key', '')
                }
                full_content = fetch_full_content_with_fallback(news_item.get("url", ""), api_config)

                if full_content:
                    news_item['raw_content'] = full_content
                    print(f"[INFO] æ–°é—» {i} å†…å®¹æŠ“å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")
                else:
                    # å†…å®¹æŠ“å–å¤±è´¥æ—¶ï¼ŒæŒ‰ä¼˜å…ˆçº§ä½¿ç”¨åŸå§‹å†…å®¹
                    fallback_content = (news_item.get('content', '') or
                                      news_item.get('description', '') or
                                      news_item.get('summary', ''))
                    news_item['raw_content'] = fallback_content
                    print(f"[WARN] æ–°é—» {i} å†…å®¹æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ (é•¿åº¦: {len(fallback_content)})")

                processed_news.append(news_item)

            except Exception as e:
                # å³ä½¿å‡ºé”™ä¹Ÿä¸ä¸¢å¼ƒæ–°é—»ï¼ŒæŒ‰ä¼˜å…ˆçº§ä½¿ç”¨åŸå§‹å†…å®¹
                fallback_content = (news_item.get('content', '') or
                                  news_item.get('description', '') or
                                  news_item.get('summary', ''))
                news_item['raw_content'] = fallback_content
                processed_news.append(news_item)
                print(f"[ERROR] æ–°é—» {i} FireCrawlè·å–å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ (é•¿åº¦: {len(fallback_content)})")

        print(f"[INFO] FireCrawlå¤„ç†å®Œæˆï¼Œ{len(processed_news)}/{len(news_list)} æ¡æ–°é—»å¤„ç†å®Œæˆ")
        return processed_news
    
    def optimize_and_translate_batch(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡AIä¼˜åŒ–å’Œç¿»è¯‘"""
        print(f"[INFO] æ­¥éª¤4ï¼šAIä¼˜åŒ–å’Œç¿»è¯‘ {len(news_list)} æ¡æ–°é—»å†…å®¹...")
        processed_news = []
        
        for i, news_item in enumerate(news_list, 1):
            try:
                title = news_item.get('title', '')[:50]
                print(f"[INFO] AIä¼˜åŒ–ç¬¬ {i}/{len(news_list)} æ¡æ–°é—»: {title}...")
                
                # ä½¿ç”¨AIä¼˜åŒ–å’Œç¿»è¯‘å†…å®¹
                raw_content = news_item.get('raw_content', '')
                original_link = news_item.get('url', '')
                
                optimized_result = self.ai_analyzer.optimize_and_translate_content(
                    raw_content, original_link
                )
                
                if optimized_result and isinstance(optimized_result, dict):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯AIæ ‡è®°çš„æ— æ•ˆæ•°æ®
                    if optimized_result.get("invalid_data", False):
                        print(f"[WARN] æ–°é—» {i} è¢«AIåˆ¤å®šä¸ºæ— æ•ˆæ•°æ®ï¼Œä¸¢å¼ƒè¯¥æ–°é—»")
                        continue

                    # éªŒè¯AIä¼˜åŒ–ç»“æœçš„å†…å®¹
                    opt_title = optimized_result.get("title", "").strip()
                    opt_content = optimized_result.get("content", "").strip()

                    if opt_title and opt_content:
                        # AIä¼˜åŒ–æˆåŠŸä¸”å†…å®¹å®Œæ•´
                        webhook_data = {
                            "message_type": optimized_result.get("message_type", "text"),
                            "title": opt_title,
                            "content": opt_content,
                            "original_link": optimized_result.get("original_link", original_link),
                            "ai_score": news_item.get('ai_score', 0.0)
                        }
                        processed_news.append(webhook_data)
                        print(f"[INFO] æ–°é—» {i} AIä¼˜åŒ–æˆåŠŸ")
                    else:
                        # AIä¼˜åŒ–è¿”å›äº†ç»“æœä½†å†…å®¹ä¸ºç©ºï¼Œç›´æ¥ä¸¢å¼ƒæ–°é—»
                        print(f"[WARN] æ–°é—» {i} AIä¼˜åŒ–è¿”å›ç©ºå†…å®¹ï¼Œä¸¢å¼ƒè¯¥æ–°é—»")
                        continue
                elif optimized_result is None:
                    # AIä¼˜åŒ–å®Œå…¨å¤±è´¥ï¼Œç›´æ¥ä¸¢å¼ƒæ–°é—»
                    print(f"[WARN] æ–°é—» {i} AIä¼˜åŒ–å®Œå…¨å¤±è´¥ï¼Œä¸¢å¼ƒè¯¥æ–°é—»")
                    continue
                else:
                    # AIä¼˜åŒ–è¿”å›äº†éå­—å…¸ç»“æœï¼Œç›´æ¥ä¸¢å¼ƒæ–°é—»
                    print(f"[WARN] æ–°é—» {i} AIä¼˜åŒ–è¿”å›å¼‚å¸¸ç»“æœï¼Œä¸¢å¼ƒè¯¥æ–°é—»")
                    continue

            except Exception as e:
                print(f"[ERROR] æ–°é—» {i} å¤„ç†å‡ºé”™: {str(e)}")
                continue
        
        return processed_news


    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'ai_analyzer_ready': bool(self.ai_analyzer),
            'gemini_enabled': bool(self.config.get('gemini_api_keys')),
            'openrouter_enabled': bool(self.config.get('openrouter_api_keys')),
            'content_extraction_enabled': self.config.get('content_extraction', {}).get('enabled', True),
            'firecrawl_enabled': bool(self.config.get('firecrawl_api_key')),
            'zyte_enabled': bool(self.config.get('zyte_api_key')),
            'min_relevance_score': self.config.get('ai_settings', {}).get('min_relevance_score', 6)
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from .config_manager import ConfigManager
    
    print("ğŸ§ª æµ‹è¯•æ–°é—»å¤„ç†å™¨...")
    
    try:
        config_manager = ConfigManager()
        config = config_manager.load_config()
        
        processor = NewsProcessor(config)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_processing_stats()
        print("ğŸ“Š å¤„ç†å™¨ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        print("\nâœ… æ–°é—»å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
