#!/usr/bin/env python3
"""
æ–°é—»å»é‡åˆå¹¶å™¨ - æ£€æµ‹ç›¸ä¼¼æ–°é—»å¹¶åˆå¹¶å†…å®¹
"""

from typing import List, Dict, Any, Tuple
import re
from difflib import SequenceMatcher
from collections import defaultdict


class NewsDeduplicator:
    """æ–°é—»å»é‡åˆå¹¶å™¨"""
    
    def __init__(self, similarity_threshold: float = 0.8):
        """
        åˆå§‹åŒ–å»é‡å™¨
        
        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯é‡å¤æ–°é—»
        """
        self.similarity_threshold = similarity_threshold
        
    def deduplicate_and_merge(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡å¹¶åˆå¹¶ç›¸ä¼¼æ–°é—»
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        if not news_list:
            return []
        
        print(f"[INFO] å¼€å§‹æ–°é—»å»é‡ï¼ŒåŸå§‹æ•°é‡: {len(news_list)}")
        
        # æ­¥éª¤1: æ‰¾å‡ºç›¸ä¼¼æ–°é—»ç»„
        similar_groups = self._find_similar_groups(news_list)
        
        # æ­¥éª¤2: åˆå¹¶æ¯ç»„ç›¸ä¼¼æ–°é—»
        merged_news = []
        processed_indices = set()
        
        for group in similar_groups:
            if len(group) > 1:
                # åˆå¹¶ç›¸ä¼¼æ–°é—»
                merged_item = self._merge_similar_news([news_list[i] for i in group])
                merged_news.append(merged_item)
                processed_indices.update(group)
                
                titles = [news_list[i].get('title', '')[:30] for i in group]
                print(f"[INFO] åˆå¹¶ {len(group)} æ¡ç›¸ä¼¼æ–°é—»: {titles}")
            else:
                # å•ç‹¬çš„æ–°é—»ï¼Œç›´æ¥æ·»åŠ 
                merged_news.append(news_list[group[0]])
                processed_indices.add(group[0])
        
        # æ·»åŠ æœªå¤„ç†çš„æ–°é—»
        for i, news in enumerate(news_list):
            if i not in processed_indices:
                merged_news.append(news)
        
        print(f"[INFO] å»é‡å®Œæˆï¼Œæœ€ç»ˆæ•°é‡: {len(merged_news)}")
        return merged_news
    
    def _find_similar_groups(self, news_list: List[Dict[str, Any]]) -> List[List[int]]:
        """æ‰¾å‡ºç›¸ä¼¼æ–°é—»ç»„"""
        groups = []
        processed = set()
        
        for i, news1 in enumerate(news_list):
            if i in processed:
                continue
                
            current_group = [i]
            processed.add(i)
            
            for j, news2 in enumerate(news_list[i+1:], i+1):
                if j in processed:
                    continue
                    
                similarity = self._calculate_similarity(news1, news2)
                if similarity >= self.similarity_threshold:
                    current_group.append(j)
                    processed.add(j)
            
            groups.append(current_group)
        
        return groups
    
    def _calculate_similarity(self, news1: Dict[str, Any], news2: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸¤æ¡æ–°é—»çš„ç›¸ä¼¼åº¦"""
        # è·å–æ ‡é¢˜å’Œå†…å®¹
        title1 = self._clean_text(news1.get('title', ''))
        title2 = self._clean_text(news2.get('title', ''))
        content1 = self._clean_text(news1.get('description', '') or news1.get('content', ''))
        content2 = self._clean_text(news2.get('description', '') or news2.get('content', ''))
        
        # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆæƒé‡70%ï¼‰
        title_similarity = self._text_similarity(title1, title2)
        
        # è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦ï¼ˆæƒé‡30%ï¼‰
        content_similarity = self._text_similarity(content1, content2)
        
        # ç»¼åˆç›¸ä¼¼åº¦
        overall_similarity = title_similarity * 0.7 + content_similarity * 0.3
        
        return overall_similarity
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼"""
        if not text:
            return ""
        
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _merge_similar_news(self, similar_news: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶ç›¸ä¼¼æ–°é—»"""
        if not similar_news:
            return {}
        
        if len(similar_news) == 1:
            return similar_news[0]
        
        # é€‰æ‹©æœ€å®Œæ•´çš„æ–°é—»ä½œä¸ºåŸºç¡€
        base_news = self._select_best_news(similar_news)
        
        # åˆå¹¶æ ‡é¢˜ï¼ˆé€‰æ‹©æœ€é•¿çš„ï¼‰
        merged_title = self._merge_titles([news.get('title', '') for news in similar_news])
        
        # åˆå¹¶å†…å®¹
        merged_content = self._merge_contents(similar_news)
        
        # åˆå¹¶æ¥æºä¿¡æ¯
        merged_sources = self._merge_sources(similar_news)
        
        # é€‰æ‹©æœ€å¯é çš„é“¾æ¥
        merged_url = self._select_best_url(similar_news)
        
        # åˆ›å»ºåˆå¹¶åçš„æ–°é—»
        merged_news = base_news.copy()
        merged_news.update({
            'title': merged_title,
            'description': merged_content,
            'content': merged_content,
            'url': merged_url,
            'source': f"åˆå¹¶æ¥æº: {merged_sources}",
            'source_type': 'Merged',
            'merged_count': len(similar_news),
            'original_sources': [news.get('source', '') for news in similar_news]
        })
        
        return merged_news
    
    def _select_best_news(self, news_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é€‰æ‹©æœ€ä½³æ–°é—»ä½œä¸ºåˆå¹¶åŸºç¡€"""
        # ä¼˜å…ˆçº§ï¼šå†…å®¹é•¿åº¦ > æ¥æºå¯é æ€§ > æ—¶é—´æ–°æ—§
        best_news = news_list[0]
        best_score = 0
        
        for news in news_list:
            score = 0
            
            # å†…å®¹é•¿åº¦è¯„åˆ†
            content = news.get('description', '') or news.get('content', '')
            score += len(content) * 0.01
            
            # æ¥æºå¯é æ€§è¯„åˆ†
            source = news.get('source', '').lower()
            if any(reliable in source for reliable in ['fda', 'efsa', 'nature', 'nutraceuticals']):
                score += 50
            elif any(good in source for good in ['food', 'health', 'science']):
                score += 20
            
            # URLå¯é æ€§è¯„åˆ†
            url = news.get('url', '')
            if any(domain in url for domain in ['.gov', '.edu', 'nature.com']):
                score += 30
            
            if score > best_score:
                best_score = score
                best_news = news
        
        return best_news
    
    def _merge_titles(self, titles: List[str]) -> str:
        """åˆå¹¶æ ‡é¢˜ï¼Œé€‰æ‹©æœ€å®Œæ•´çš„"""
        if not titles:
            return ""
        
        # å»é™¤ç©ºæ ‡é¢˜
        valid_titles = [title.strip() for title in titles if title.strip()]
        
        if not valid_titles:
            return ""
        
        # é€‰æ‹©æœ€é•¿çš„æ ‡é¢˜
        return max(valid_titles, key=len)
    
    def _merge_contents(self, news_list: List[Dict[str, Any]]) -> str:
        """åˆå¹¶å†…å®¹ï¼Œå»é‡å¹¶ä¿ç•™æœ€å®Œæ•´çš„ä¿¡æ¯"""
        contents = []
        
        for news in news_list:
            content = news.get('description', '') or news.get('content', '')
            if content and content.strip():
                contents.append(content.strip())
        
        if not contents:
            return ""
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå†…å®¹ï¼Œç›´æ¥è¿”å›
        if len(contents) == 1:
            return contents[0]
        
        # é€‰æ‹©æœ€é•¿çš„å†…å®¹ä½œä¸ºä¸»è¦å†…å®¹
        main_content = max(contents, key=len)
        
        # æ£€æŸ¥å…¶ä»–å†…å®¹æ˜¯å¦æœ‰é¢å¤–ä¿¡æ¯
        additional_info = []
        for content in contents:
            if content != main_content and len(content) > 50:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸»è¦å†…å®¹ä¸­æ²¡æœ‰çš„å…³é”®ä¿¡æ¯
                if self._has_additional_info(main_content, content):
                    additional_info.append(content)
        
        # åˆå¹¶å†…å®¹
        if additional_info:
            merged = main_content + "\n\nè¡¥å……ä¿¡æ¯ï¼š" + " | ".join(additional_info[:2])  # æœ€å¤šæ·»åŠ 2ä¸ªè¡¥å……ä¿¡æ¯
            return merged
        else:
            return main_content
    
    def _has_additional_info(self, main_content: str, other_content: str) -> bool:
        """æ£€æŸ¥å…¶ä»–å†…å®¹æ˜¯å¦åŒ…å«ä¸»è¦å†…å®¹ä¸­æ²¡æœ‰çš„ä¿¡æ¯"""
        main_words = set(self._clean_text(main_content).lower().split())
        other_words = set(self._clean_text(other_content).lower().split())
        
        # å¦‚æœå…¶ä»–å†…å®¹æœ‰è¶…è¿‡30%çš„ç‹¬ç‰¹è¯æ±‡ï¼Œè®¤ä¸ºæœ‰é¢å¤–ä¿¡æ¯
        unique_words = other_words - main_words
        return len(unique_words) / len(other_words) > 0.3 if other_words else False
    
    def _merge_sources(self, news_list: List[Dict[str, Any]]) -> str:
        """åˆå¹¶æ¥æºä¿¡æ¯"""
        sources = []
        for news in news_list:
            source = news.get('source', '') or news.get('source_type', '')
            if source and source not in sources:
                sources.append(source)
        
        return ", ".join(sources[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ¥æº
    
    def _select_best_url(self, news_list: List[Dict[str, Any]]) -> str:
        """é€‰æ‹©æœ€å¯é çš„URL"""
        urls = [news.get('url', '') for news in news_list if news.get('url')]
        
        if not urls:
            return ""
        
        # ä¼˜å…ˆé€‰æ‹©æƒå¨åŸŸå
        for url in urls:
            if any(domain in url for domain in ['.gov', '.edu', 'nature.com', 'fda.gov', 'efsa.europa.eu']):
                return url
        
        # å…¶æ¬¡é€‰æ‹©ä¸“ä¸šåª’ä½“
        for url in urls:
            if any(domain in url for domain in ['nutraceuticals', 'fooddive', 'wholefoodsmagazine']):
                return url
        
        # æœ€åè¿”å›ç¬¬ä¸€ä¸ªURL
        return urls[0]


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯•æ–°é—»å»é‡åˆå¹¶å™¨...")
    
    # æ¨¡æ‹Ÿç›¸ä¼¼æ–°é—»
    test_news = [
        {
            "title": "New Study Shows Benefits of Grape Seed Extract",
            "description": "A recent study published in Nature shows that grape seed extract has antioxidant properties.",
            "url": "https://example1.com",
            "source": "Nature"
        },
        {
            "title": "Study: Grape Seed Extract Shows Antioxidant Benefits",
            "description": "Research published in Nature demonstrates the antioxidant effects of grape seed extract in clinical trials.",
            "url": "https://example2.com",
            "source": "Health News"
        },
        {
            "title": "FDA Approves New Dietary Supplement Regulations",
            "description": "The FDA has announced new regulations for dietary supplements.",
            "url": "https://fda.gov/news",
            "source": "FDA"
        }
    ]
    
    deduplicator = NewsDeduplicator(similarity_threshold=0.7)
    merged_news = deduplicator.deduplicate_and_merge(test_news)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"åŸå§‹æ–°é—»æ•°é‡: {len(test_news)}")
    print(f"å»é‡åæ•°é‡: {len(merged_news)}")
    
    for i, news in enumerate(merged_news, 1):
        print(f"\næ–°é—» {i}:")
        print(f"  æ ‡é¢˜: {news.get('title', '')}")
        print(f"  æ¥æº: {news.get('source', '')}")
        if news.get('merged_count', 0) > 1:
            print(f"  åˆå¹¶æ•°é‡: {news.get('merged_count')} æ¡")
            print(f"  åŸå§‹æ¥æº: {news.get('original_sources', [])}")
